{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7eaa9ae1",
   "metadata": {},
   "source": [
    "## Import The relevant packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df8631d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e9c123",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be126fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining Values\n",
    "\n",
    "DATASET_SIZE = 70000\n",
    "TRAIN_RATIO = 0.8\n",
    "VALIDATION_RATIO = 0.1\n",
    "TEST_RATIO = 0.1\n",
    "\n",
    "# Loading MNIST dataset and shuffling \n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "X = np.concatenate([x_train, x_test])\n",
    "y = np.concatenate([y_train, y_test])\n",
    "\n",
    "BUFFER_SIZE = 1000\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((X, y))\n",
    "dataset = dataset.shuffle(BUFFER_SIZE)\n",
    "\n",
    "# Scaling the data by dividing each and every value by 255.\n",
    "def scale(image, label):\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    image /= 255.\n",
    "    return image, label\n",
    "\n",
    "\n",
    "# splitting Train, Validation, and Test sets and Scaling them using .map() function\n",
    "\n",
    "train_dataset = (dataset.take(int(TRAIN_RATIO*DATASET_SIZE))).map(scale)\n",
    "validation_dataset = (dataset.skip(int(TRAIN_RATIO*DATASET_SIZE)).take(int(VALIDATION_RATIO*DATASET_SIZE))).map(scale)\n",
    "test_dataset = (dataset.skip(int(TRAIN_RATIO*DATASET_SIZE)).skip(int(VALIDATION_RATIO*DATASET_SIZE))).map(scale)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16321dd4",
   "metadata": {},
   "source": [
    "### Creating batches for mini-batch gredient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "caeae57a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating batches it the Train and Validation datasets to use mini-batch gradient descent\n",
    "\n",
    "Batch_size = 10\n",
    "\n",
    "train_data = train_dataset.batch(Batch_size)\n",
    "\n",
    "num_validation_samples = tf.cast(VALIDATION_RATIO * DATASET_SIZE, tf.int64)\n",
    "validation_data = validation_dataset.batch(num_validation_samples)\n",
    "\n",
    "num_test_samples = tf.cast(TEST_RATIO * DATASET_SIZE, tf.int64)\n",
    "test_data = test_dataset.batch(num_test_samples)\n",
    "\n",
    "validation_inputs, validation_targets = next(iter(validation_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b853f24",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b2a8f5",
   "metadata": {},
   "source": [
    "### outline the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cef2b29b",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 784\n",
    "output_size = 10\n",
    "hidden_layer_size = 200\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "                            # flatten input to convert into a vector\n",
    "                            tf.keras.layers.Flatten(input_shape=(28,28,1)),\n",
    "    \n",
    "                            # using .Dense() method to calculate the dot product of weights and inputs and add biases. it can also be used to apply activation function.\n",
    "                            tf.keras.layers.Dense(hidden_layer_size, activation= 'relu'),\n",
    "                            tf.keras.layers.Dense(hidden_layer_size, activation= 'tanh'),\n",
    "                            tf.keras.layers.Dense(output_size, activation= 'softmax') \n",
    "                            ])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ffd0d1d",
   "metadata": {},
   "source": [
    "### Choose the optimizer and loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b9c36855",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_optimizer = tf.keras.optimizers.SGD(learning_rate=0.01)\n",
    "model.compile(optimizer=custom_optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df29a762",
   "metadata": {},
   "source": [
    "### Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b5ddccd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "5600/5600 - 5s - loss: 0.3827 - accuracy: 0.8947 - val_loss: 0.2149 - val_accuracy: 0.9370 - 5s/epoch - 957us/step\n",
      "Epoch 2/5\n",
      "5600/5600 - 5s - loss: 0.1894 - accuracy: 0.9444 - val_loss: 0.1550 - val_accuracy: 0.9556 - 5s/epoch - 882us/step\n",
      "Epoch 3/5\n",
      "5600/5600 - 5s - loss: 0.1361 - accuracy: 0.9597 - val_loss: 0.1204 - val_accuracy: 0.9663 - 5s/epoch - 902us/step\n",
      "Epoch 4/5\n",
      "5600/5600 - 5s - loss: 0.1052 - accuracy: 0.9696 - val_loss: 0.1076 - val_accuracy: 0.9691 - 5s/epoch - 922us/step\n",
      "Epoch 5/5\n",
      "5600/5600 - 5s - loss: 0.0860 - accuracy: 0.9747 - val_loss: 0.0953 - val_accuracy: 0.9737 - 5s/epoch - 894us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a19c78b130>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_epochs = 5\n",
    "\n",
    "model.fit(train_data, epochs = num_epochs, validation_data = (validation_inputs, validation_targets), verbose = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05994f7b",
   "metadata": {},
   "source": [
    "### Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "44224c2a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 171ms/step - loss: 0.0792 - accuracy: 0.9764\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = model.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "38192f32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.08. Test accuracy: 97.64%\n"
     ]
    }
   ],
   "source": [
    "print('Test loss: {0:.2f}. Test accuracy: {1:.2f}%'.format(test_loss, test_accuracy * 100.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b3dcfbb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c604fcd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
